{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d27eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dc04078",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a623ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER=\"/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca5cbfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def massiveweb_filter(text,    \n",
    "                      min_words=50,    \n",
    "                      max_words=100_000,    \n",
    "                      min_word_chars=3,   \n",
    "                      max_word_chars=10,    \n",
    "                      max_symbol_to_word_ratio=0.1,    \n",
    "                      max_lines_end_ellipsis_pct=0.3,    \n",
    "                      min_words_alphabetic_char_pct=0.9,    \n",
    "                      lang=\"pt\",\n",
    "                     ):\n",
    "    \n",
    "    \"\"\"Apply the same cleaning rules used to construct the MassiveWeb corpus. The rules are described in Appendix A.1.1\n",
    "    of Gopher's paper (https://arxiv.org/pdf/2112.11446.pdf).\n",
    "\n",
    "    Returns 0 in case all rules passed. Otherwise, returns an integer that indicates which rule was not satisfied.\n",
    "    \"\"\"\n",
    "    # TODO: use NLTK to get words?\n",
    "\n",
    "    # remove multiple spaces, line breaks and tabs to get actual \"words\".\n",
    "    words = ' '.join(text.split()).split()  \n",
    "    num_words = len(words)\n",
    "\n",
    "    # Remove any document that have less than min_words or more than max_words.\n",
    "    if num_words < min_words or num_words > max_words:\n",
    "        return 1\n",
    "\n",
    "    # Remove any document whose mean word length in chars is less than min_word_chars or more than max_word_chars.\n",
    "    mean_word_chars = np.mean(list(map(len, words)))\n",
    "    if mean_word_chars < min_word_chars or mean_word_chars > max_word_chars:\n",
    "        return 2\n",
    "\n",
    "    # Remove any document with a symbol-to-word ratio greater than max_symbol_to_word_ratio for either the hash symbol\n",
    "    # or the ellipsis.\n",
    "    num_symbols = np.sum([word == '#' or word == '...' for word in words])\n",
    "    if num_symbols / (num_words - num_symbols) > max_symbol_to_word_ratio:\n",
    "        return 3\n",
    "\n",
    "    # Remove any document with more than 90% of lines starting with a bullet point.\n",
    "    # TODO: How to get bullet points if we are already using clean text?\n",
    "\n",
    "    # Remove any document with more than 30% of lines ending with an ellipsis.\n",
    "    lines = text.split('\\n')\n",
    "    if np.mean(list(map(lambda line: line.endswith('...'), lines))) > max_lines_end_ellipsis_pct:\n",
    "        return 4\n",
    "    \n",
    "    # Remove any document that has less than 80% of words with at least one alphabetic character.\n",
    "    # TODO: use regex to make it faster? E.g.: re.search('[a-zA-Z]', word)\n",
    "    if np.mean([any(char.isalpha() for char in word) for word in words]) < min_words_alphabetic_char_pct:\n",
    "        return 5\n",
    "\n",
    "    # Remove any document that does not contain at least two stop words.\n",
    "    if lang == 'pt':\n",
    "        stop_words = set(['a', 'com', 'e', 'Ã©', 'de', 'o', 'para', 'que', 'tem'])\n",
    "    else:\n",
    "        stop_words = set(['the', 'be', 'to', 'of', 'and', 'that', 'have', 'with'])\n",
    "\n",
    "    if sum(stop_word in words for stop_word in stop_words) < 2:\n",
    "        return 6\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c40eb6",
   "metadata": {},
   "source": [
    "### Apply MassiveWeb filter over all cleaned passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0f3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parts=np.sort(glob.glob(os.path.join(DATASET_FOLDER, \"clueweb22-pt_colbertx_0*_cleaned.tsv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f8e58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_00_cleaned.tsv',\n",
       "       '/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_01_cleaned.tsv',\n",
       "       '/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_02_cleaned.tsv',\n",
       "       '/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_03_cleaned.tsv',\n",
       "       '/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_04_cleaned.tsv'],\n",
       "      dtype='<U125')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb642a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for part in dataset_parts:\n",
    "    print(\"Handling {}...\".format(os.path.basename(part)))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    part_df = pd.read_csv(part, sep='\\t', names=['passage_id', 'passage', 'n_tokens'])\n",
    "\n",
    "    print(\">> part_df.shape={}\".format(part_df.shape))\n",
    "    \n",
    "    part_df['massiveweb_filter'] = part_df['passage'].apply(lambda x: massiveweb_filter(x))\n",
    "    \n",
    "    filtered_out_count = np.sum(part_df['massiveweb_filter'] > 0)\n",
    "    \n",
    "    print(\">> number of passages filtered out: {} ({:.4}%)\".format(filtered_out_count, filtered_out_count/part_df.shape[0]))\n",
    "    \n",
    "    filter_results = np.unique(part_df['massiveweb_filter'].to_numpy(), return_counts=True)\n",
    "    \n",
    "    print(filter_results)\n",
    "    \n",
    "    print(\"\\n>> elapsed time: {}\\n\".format(time.time() - start_time))\n",
    "    \n",
    "    output_file = os.path.join(DATASET_FOLDER, \"{}_massiveweb.tsv\".format(os.path.splitext(os.path.basename(part))[0]))\n",
    "    \n",
    "    print(\">> Saving result as {}\\n\\n\".format(output_file))\n",
    "    \n",
    "    part_df.to_csv(output_file, sep='\\t', index=False)\n",
    "    \n",
    "    results.append({\"file\": os.path.basename(part),\n",
    "                    \"total_passages\": part_df.shape[0],\n",
    "                    \"removed_passages\": filtered_out_count,\n",
    "                    \"filter_results\": filter_results})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabd0cc",
   "metadata": {},
   "source": [
    "### Sample 1M passages from the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8331ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_samples(which_file, selected_passages, starting_position=0, dataset_position=0, output_folder=\"\", add_headers=False, filter_field=None, filter_values_list=None):\n",
    "    if add_headers:\n",
    "        dataset_df = pd.read_csv(which_file, sep=\"\\t\", names=['passage_id', 'passage', 'n_tokens', 'massiveweb_filter'])\n",
    "    else:\n",
    "        dataset_df = pd.read_csv(which_file, sep=\"\\t\")\n",
    "    \n",
    "    print(\"Dataset original shape: {}\".format(dataset_df.shape))\n",
    "    \n",
    "    if filter_field is not None:\n",
    "        dataset_df = dataset_df[np.isin(dataset_df[filter_field], filter_values_list)]\n",
    "        \n",
    "        print(\"Filtered dataset shape: {}\".format(dataset_df.shape))\n",
    "    \n",
    "    \n",
    "    first_higher_list = np.where(selected_passages[starting_position:] >= dataset_position + dataset_df.shape[0])[0]\n",
    "    \n",
    "    if first_higher_list.shape[0] > 0:\n",
    "        first_higher = first_higher_list[0]\n",
    "    else:\n",
    "        first_higher = selected_passages.shape[0] - starting_position\n",
    "    \n",
    "    print(\"Selecting {} passages in this part, from {}({}) until {}({})\".format(first_higher, \n",
    "                                                                                starting_position, selected_passages[starting_position],\n",
    "                                                                                starting_position + first_higher - 1, selected_passages[starting_position + first_higher - 1]))\n",
    "    \n",
    "    print(selected_passages[starting_position:(starting_position + first_higher)] - dataset_position)\n",
    "    \n",
    "    selected_dataset_df = dataset_df.iloc[selected_passages[starting_position:(starting_position + first_higher)] - dataset_position]\n",
    "    \n",
    "    print(\"Selected dataset shape: {}\".format(selected_dataset_df.shape))\n",
    "    \n",
    "    output_filename = '{}_sample.tsv'.format(os.path.splitext(os.path.basename(which_file))[0])\n",
    "    \n",
    "    print(\"Saving resulting DF as {}...\\n\".format(output_filename))\n",
    "    \n",
    "    selected_dataset_df.to_csv(os.path.join(output_folder, output_filename), sep=\"\\t\", header=None, index=False)\n",
    "   \n",
    "    return selected_dataset_df, starting_position + first_higher, dataset_df.shape[0] + dataset_position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc31577",
   "metadata": {},
   "source": [
    "#### Temp code to generate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db056349",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "\n",
    "for cleaned_part in cleaned_collection_parts:\n",
    "    dataset_df = pd.read_csv(cleaned_part, sep=\"\\t\")\n",
    "    \n",
    "    filter_results = np.unique(dataset_df['massiveweb_filter'].to_numpy(), return_counts=True)\n",
    "    \n",
    "    results.append({'filter_results': filter_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1302a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filter_results': (array([0, 1, 2, 3, 4, 5, 6]),\n",
       "   array([1412134,   59395,    7178,     310,   10525,  346475,   16441]))},\n",
       " {'filter_results': (array([0, 1, 2, 3, 4, 5, 6]),\n",
       "   array([1412696,   59228,    8378,     284,   10353,  344613,   16796]))},\n",
       " {'filter_results': (array([0, 1, 2, 3, 4, 5, 6]),\n",
       "   array([1413085,   59256,    7226,     286,   10161,  344014,   16710]))},\n",
       " {'filter_results': (array([0, 1, 2, 3, 4, 5, 6]),\n",
       "   array([1415593,   58972,    8594,     270,   10336,  343488,   16795]))},\n",
       " {'filter_results': (array([0, 1, 2, 3, 4, 5, 6]),\n",
       "   array([1414146,   59856,    7489,     286,   10416,  344440,   16778]))}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc40374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_passages_kept = np.sum([np.sum(part['filter_results'][1][[0, 4]]) for part in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7674bcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7119445"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_passages_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2c11c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1422659, 1423049, 1423246, 1425929, 1424562]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.sum(part['filter_results'][1][[0, 4]]) for part in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac7a2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_passages = np.sort(np.random.choice(list(range(0, total_passages_kept)), int(1e+6), replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2b3c12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_passages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a94fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_collection_parts = np.sort(glob.glob(os.path.join(DATASET_FOLDER, \"clueweb22-pt_colbertx_0*_cleaned_massiveweb.tsv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eeed822b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_00_cleaned_massiveweb.tsv',\n",
       "       '/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_01_cleaned_massiveweb.tsv',\n",
       "       '/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_02_cleaned_massiveweb.tsv',\n",
       "       '/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_03_cleaned_massiveweb.tsv',\n",
       "       '/media/eduseiti/bigdata01/unicamp/ia368v_dd/trabalho_final/clueweb22-pt_10M_sample_fixed/clueweb22-pt_colbertx_04_cleaned_massiveweb.tsv'],\n",
       "      dtype='<U136')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_collection_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03506cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original shape: (1852458, 4)\n",
      "Filtered dataset shape: (1422659, 4)\n",
      "Selecting 199972 passages in this part, from 0(3) until 199971(1422655)\n",
      "[      3       4       5 ... 1422652 1422653 1422655]\n",
      "Selected dataset shape: (199972, 4)\n",
      "Saving resulting DF as clueweb22-pt_colbertx_00_cleaned_massiveweb_sample.tsv...\n",
      "\n",
      "Dataset original shape: (1852348, 4)\n",
      "Filtered dataset shape: (1423049, 4)\n",
      "Selecting 199800 passages in this part, from 199972(1422662) until 399771(2845707)\n",
      "[      3       5       6 ... 1423043 1423045 1423048]\n",
      "Selected dataset shape: (199800, 4)\n",
      "Saving resulting DF as clueweb22-pt_colbertx_01_cleaned_massiveweb_sample.tsv...\n",
      "\n",
      "Dataset original shape: (1850738, 4)\n",
      "Filtered dataset shape: (1423246, 4)\n",
      "Selecting 199400 passages in this part, from 399772(2845710) until 599171(4268953)\n",
      "[      2       6       9 ... 1423239 1423242 1423245]\n",
      "Selected dataset shape: (199400, 4)\n",
      "Saving resulting DF as clueweb22-pt_colbertx_02_cleaned_massiveweb_sample.tsv...\n",
      "\n",
      "Dataset original shape: (1854048, 4)\n",
      "Filtered dataset shape: (1425929, 4)\n",
      "Selecting 199995 passages in this part, from 599172(4268955) until 799166(5694880)\n",
      "[      1      14      23 ... 1425915 1425924 1425926]\n",
      "Selected dataset shape: (199995, 4)\n",
      "Saving resulting DF as clueweb22-pt_colbertx_03_cleaned_massiveweb_sample.tsv...\n",
      "\n",
      "Dataset original shape: (1853411, 4)\n",
      "Filtered dataset shape: (1424562, 4)\n",
      "Selecting 200833 passages in this part, from 799167(5694890) until 999999(7119441)\n",
      "[      7       9      19 ... 1424544 1424551 1424558]\n",
      "Selected dataset shape: (200833, 4)\n",
      "Saving resulting DF as clueweb22-pt_colbertx_04_cleaned_massiveweb_sample.tsv...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "starting_position = 0\n",
    "dataset_position = 0\n",
    "\n",
    "final_passages_count = 0\n",
    "\n",
    "for cleaned_part in cleaned_collection_parts:\n",
    "    final_results = select_samples(cleaned_part, \n",
    "                                   selected_passages, \n",
    "                                   starting_position, \n",
    "                                   dataset_position, \n",
    "                                   output_folder=DATASET_FOLDER,\n",
    "                                   filter_field=\"massiveweb_filter\",\n",
    "                                   filter_values_list=[0, 4])\n",
    "    \n",
    "    starting_position = final_results[1]\n",
    "    dataset_position = final_results[2]\n",
    "    \n",
    "    final_passages_count += final_results[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "792e4ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_passages_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c426fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2143558"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9263003 - 7119445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31bc3439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7685893008995031"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7119445 / 9263003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a3cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
