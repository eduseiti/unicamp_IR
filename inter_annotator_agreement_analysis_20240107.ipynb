{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13aaf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "from scipy.stats import spearmanr, kendalltau, pearsonr\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c23117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.6f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77d30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_FOLDER=\"../trabalho_final/anotações_humanas\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca165d9",
   "metadata": {},
   "source": [
    "### Mapping 4-score to 3-score evaluations to match GPT-4 3-score evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4c9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapper = {\n",
    "    0:0,\n",
    "    1:0,\n",
    "    2:1,\n",
    "    3:2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4f127",
   "metadata": {},
   "source": [
    "### Original 4-score to 3-score evaluations mapping\n",
    "\n",
    "This mapping changes the original 4-score 1 evaluation meaning, indicating it **partially answers to the question**, while the original (TREC-DL 21) meaning is the passage **does not answer to the question**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb766ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapper_original = {\n",
    "    0:0,\n",
    "    1:1,\n",
    "    2:1,\n",
    "    3:2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade176f7",
   "metadata": {},
   "source": [
    "### Mapping 4-score to 2-score evenly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc56c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapper_for_2 = {\n",
    "    0:0,\n",
    "    1:0,\n",
    "    2:1,\n",
    "    3:1    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f801f68",
   "metadata": {},
   "source": [
    "### Alternative version for mapping 4-score to 2-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e27068",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapper_for_2_alt = {\n",
    "    0:0,\n",
    "    1:1,\n",
    "    2:1,\n",
    "    3:1    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86e2ce",
   "metadata": {},
   "source": [
    "### Mapping 3-score to 2-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ca27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mapper_3_for_2 = {\n",
    "    0:0,\n",
    "    1:1,\n",
    "    2:1    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c312ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_agreement_per_questions(evaluation_a, evaluation_b, suffix=None, score='score'):\n",
    "    \n",
    "    merged_df = evaluation_a.merge(evaluation_b, left_on='doccano_id', right_on='doccano_id')[['query_x', 'passage_x', 'passage_id_x', score + '_x', score + '_y']]\n",
    "    \n",
    "    correlations = []\n",
    "    \n",
    "    for group_name, group_df in merged_df.groupby('query_x', sort=False):\n",
    "        correlations.append({'query': group_name,\n",
    "                             'cohen_kappa{}'.format(suffix): cohen_kappa_score(group_df[score + '_x'], group_df[score + '_y'])})\n",
    "        \n",
    "    return merged_df, pd.DataFrame(correlations).fillna(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b6c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_data_consolidated(correlation_df, columns_to_plot, plot_title):\n",
    "    \n",
    "    fig = plt.figure(figsize=[10, 10])\n",
    "\n",
    "    fig.suptitle(plot_title, y=0.91)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    all_boxplots = plt.boxplot(correlation_df[columns_to_plot], patch_artist=True, vert=False, meanline=True, showmeans=True)\n",
    "\n",
    "    colors = ['pink', 'lightblue', 'lightgreen']\n",
    "    \n",
    "    all_boxes = all_boxplots['boxes']\n",
    "    \n",
    "    print(all_boxes)\n",
    "    \n",
    "    for i, which_box in enumerate(all_boxes):\n",
    "        which_box.set_facecolor(colors[i // 3])\n",
    "    \n",
    "    \n",
    "    plt.yticks(range(1, len(columns_to_plot) + 1), columns_to_plot)\n",
    "    plt.xticks(np.arange(-0.3, 1.0, 0.1))\n",
    "    \n",
    "    plt.legend(handles=all_boxes[::3], labels=[\"Humanos x Humanos\", \"Humanos x GPT3.5\", \"Humanos x GPT4\"], bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f154b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_data(correlation_df, columns_to_plot, plot_title):\n",
    "    \n",
    "    fig = plt.figure(figsize=[15, 40])\n",
    "\n",
    "    fig.suptitle(plot_title, y=0.91)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    all_boxplots = plt.boxplot(correlation_df[columns_to_plot].to_numpy().transpose(), patch_artist=True, vert=False, meanline=True, showmeans=True)\n",
    "\n",
    "    colors = ['pink', 'lightblue', 'lightgreen', 'lightyellow'][::-1]\n",
    "    \n",
    "    all_boxes = all_boxplots['boxes']\n",
    "    \n",
    "#     print(all_boxes)\n",
    "    \n",
    "    for i, which_box in enumerate(all_boxes):\n",
    "        which_box.set_facecolor(colors[i % 4])\n",
    "\n",
    "        \n",
    "    \n",
    "    plt.yticks(range(4, correlation_df.shape[0] + 1, 4), correlation_df.iloc[::4]['query'])\n",
    "    plt.xticks(np.arange(-0.3, 1.0, 0.1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4266482",
   "metadata": {},
   "source": [
    "## Read human annotators evaluations and map the original 4-score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28e99c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_df = pd.read_csv(os.path.join(ANNOTATIONS_FOLDER, \"admin_240_annotations_with_questions.tsv\"), sep='\\t')\n",
    "a2_df = pd.read_csv(os.path.join(ANNOTATIONS_FOLDER, \"Eduardo_240_annotations_with_questions.tsv\"), sep='\\t')\n",
    "a3_df = pd.read_csv(os.path.join(ANNOTATIONS_FOLDER, \"Leodecio_240_annotations_with_questions.tsv\"), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d255ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_df['score'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85575421",
   "metadata": {},
   "source": [
    "### Compute single score based on the human annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cf20c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e0a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_df['a1'] = a1_df['score']\n",
    "all_scores_df['a2'] = a2_df['score']\n",
    "all_scores_df['a3'] = a3_df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4771757f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     a1  a2  a3\n",
       "0     3   3   3\n",
       "1     0   0   0\n",
       "2     2   1   1\n",
       "3     2   0   0\n",
       "4     3   2   1\n",
       "..   ..  ..  ..\n",
       "235   2   2   1\n",
       "236   3   0   1\n",
       "237   1   3   3\n",
       "238   1   0   0\n",
       "239   2   0   0\n",
       "\n",
       "[240 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "455bdb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_score = []\n",
    "\n",
    "for i, row in all_scores_df.iterrows():\n",
    "    if (row['a1'] == row['a2']) or (row['a1'] == row['a3']):\n",
    "        single_score.append(row['a1'])\n",
    "\n",
    "    elif row['a2'] == row['a3']:\n",
    "        single_score.append(row['a2'])\n",
    "        \n",
    "    else:\n",
    "        single_score.append(np.random.choice([row['a1'], row['a2'], row['a3']], 1)[0])\n",
    "            \n",
    "single_score = np.array(single_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d77c77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 1, 0, 3, 0, 0, 0, 1, 0, 3, 3, 2, 2, 1, 1, 1, 0, 1, 0, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 2, 0, 0, 2, 0, 1, 2, 2, 0, 1, 0, 3, 2, 3, 3,\n",
       "       0, 2, 0, 0, 0, 0, 3, 2, 3, 1, 2, 3, 1, 1, 2, 2, 2, 1, 3, 3, 3, 3,\n",
       "       2, 3, 1, 3, 3, 3, 2, 3, 2, 2, 1, 3, 2, 2, 0, 0, 2, 2, 2, 0, 3, 1,\n",
       "       1, 1, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 1, 0, 3, 3, 2, 3, 2,\n",
       "       0, 2, 1, 1, 2, 0, 0, 2, 1, 2, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 1, 2,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 0, 0, 1, 1, 3, 3, 1, 1,\n",
       "       2, 0, 2, 0, 1, 0, 0, 1, 2, 2, 2, 1, 0, 2, 2, 2, 3, 2, 2, 2, 2, 1,\n",
       "       1, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 2, 2, 2, 1, 0, 1,\n",
       "       2, 1, 0, 3, 0, 1, 1, 0, 0, 0, 1, 0, 2, 3, 3, 2, 3, 2, 1, 3, 1, 3,\n",
       "       3, 3, 1, 2, 2, 3, 3, 0, 0, 0, 1, 0, 2, 0, 0, 2, 1, 3, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "158b4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_score_df = pd.DataFrame(single_score, columns=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c4f6db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     score\n",
       "0        3\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        3\n",
       "..     ...\n",
       "235      2\n",
       "236      1\n",
       "237      3\n",
       "238      0\n",
       "239      0\n",
       "\n",
       "[240 rows x 1 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19a844",
   "metadata": {},
   "source": [
    "### Convert the 4-score to other ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9092e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_df['03_score'] = a1_df['score'].map(score_mapper)\n",
    "a2_df['03_score'] = a2_df['score'].map(score_mapper)\n",
    "a3_df['03_score'] = a3_df['score'].map(score_mapper)\n",
    "\n",
    "single_score_df['03_score'] = common_score_df['score'].map(score_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3112f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_df['02_score'] = a1_df['score'].map(score_mapper_for_2)\n",
    "a2_df['02_score'] = a2_df['score'].map(score_mapper_for_2)\n",
    "a3_df['02_score'] = a3_df['score'].map(score_mapper_for_2)\n",
    "\n",
    "single_score_df['02_score'] = common_score_df['score'].map(score_mapper_for_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "194fea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_df['02_score_alt'] = a1_df['score'].map(score_mapper_for_2_alt)\n",
    "a2_df['02_score_alt'] = a2_df['score'].map(score_mapper_for_2_alt)\n",
    "a3_df['02_score_alt'] = a3_df['score'].map(score_mapper_for_2_alt)\n",
    "\n",
    "single_score_df['02_score_alt'] = common_score_df['score'].map(score_mapper_for_2_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c88e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_kappa_wrapper(first_series, second_series):\n",
    "    return (cohen_kappa_score(first_series, second_series), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e31bc0",
   "metadata": {},
   "source": [
    "## Functions for correlation for 1st, 2nd and all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32207635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations_1st_set(evaluation_df, which_score, correlation_fn):\n",
    "\n",
    "    general_agreement_df = pd.DataFrame()\n",
    "\n",
    "    general_agreement_df['comparisson'] = ['a1', 'a2', 'a3']\n",
    "\n",
    "    general_agreement_df['a1'] = [np.nan, \n",
    "                                  globals()[correlation_fn](a1_df.iloc[0:120][which_score], a2_df.iloc[0:120][which_score])[0], \n",
    "                                  globals()[correlation_fn](a1_df.iloc[0:120][which_score], a3_df.iloc[0:120][which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a2'] = [globals()[correlation_fn](a1_df.iloc[0:120][which_score], a2_df.iloc[0:120][which_score])[0], \n",
    "                                  np.nan, \n",
    "                                  globals()[correlation_fn](a2_df.iloc[0:120][which_score], a3_df.iloc[0:120][which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a3'] = [globals()[correlation_fn](a1_df.iloc[0:120][which_score], a3_df.iloc[0:120][which_score])[0], \n",
    "                                  globals()[correlation_fn](a2_df.iloc[0:120][which_score], a3_df.iloc[0:120][which_score])[0], \n",
    "                                  np.nan]\n",
    "\n",
    "    general_agreement_df['GPT4 1st 4-score'] = [globals()[correlation_fn](a1_df.iloc[0:120][which_score], evaluation_df[which_score])[0], \n",
    "                                                globals()[correlation_fn](a2_df.iloc[0:120][which_score], evaluation_df[which_score])[0], \n",
    "                                                globals()[correlation_fn](a3_df.iloc[0:120][which_score], evaluation_df[which_score])[0]]\n",
    "\n",
    "    general_agreement_df = pd.concat([general_agreement_df, \n",
    "                                      pd.DataFrame(data=[['mean'] + general_agreement_df.iloc[:, 1:].mean().to_list()], columns=general_agreement_df.columns)])\n",
    "\n",
    "    general_agreement_df = pd.concat([general_agreement_df, \n",
    "                                      pd.DataFrame(data=[['std'] + general_agreement_df.iloc[:, 1:].std().to_list()], columns=general_agreement_df.columns)])    \n",
    "    \n",
    "    human_mean = general_agreement_df.iloc[-1,1:4].mean()\n",
    "\n",
    "    general_agreement_df = pd.concat([general_agreement_df,\n",
    "                                      pd.DataFrame(data=[['Difference from mean human annotators'] + (general_agreement_df.iloc[-1, 1:] - human_mean).to_list()], columns=general_agreement_df.columns)])\n",
    "\n",
    "    return general_agreement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c606f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations_2nd_set(evaluation_df, which_score, correlation_fn):\n",
    "    \n",
    "    general_agreement_df = pd.DataFrame()\n",
    "\n",
    "    general_agreement_df['comparisson'] = ['a1', 'a2', 'a3']\n",
    "\n",
    "    general_agreement_df['a1'] = [np.nan, \n",
    "                                  globals()[correlation_fn](a1_df.iloc[120:240][which_score], a2_df.iloc[120:240][which_score])[0], \n",
    "                                  globals()[correlation_fn](a1_df.iloc[120:240][which_score], a3_df.iloc[120:240][which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a2'] = [globals()[correlation_fn](a1_df.iloc[120:240][which_score], a2_df.iloc[120:240][which_score])[0], \n",
    "                                  np.nan, \n",
    "                                  globals()[correlation_fn](a2_df.iloc[120:240][which_score], a3_df.iloc[120:240][which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a3'] = [globals()[correlation_fn](a1_df.iloc[120:240][which_score], a3_df.iloc[120:240][which_score])[0], \n",
    "                                  globals()[correlation_fn](a2_df.iloc[120:240][which_score], a3_df.iloc[120:240][which_score])[0], \n",
    "                                  np.nan]\n",
    "\n",
    "    general_agreement_df['GPT4 2nd 4-score'] = [globals()[correlation_fn](a1_df.iloc[120:240][which_score], evaluation_df[which_score])[0], \n",
    "                                                globals()[correlation_fn](a2_df.iloc[120:240][which_score], evaluation_df[which_score])[0], \n",
    "                                                globals()[correlation_fn](a3_df.iloc[120:240][which_score], evaluation_df[which_score])[0]]\n",
    "\n",
    "    general_agreement_df = pd.concat([general_agreement_df, \n",
    "                                      pd.DataFrame(data=[['mean'] + general_agreement_df.iloc[:, 1:].mean().to_list()], columns=general_agreement_df.columns)])\n",
    "    \n",
    "    general_agreement_df = pd.concat([general_agreement_df, \n",
    "                                      pd.DataFrame(data=[['std'] + general_agreement_df.iloc[:, 1:].std().to_list()], columns=general_agreement_df.columns)])\n",
    "    \n",
    "    human_mean = general_agreement_df.iloc[-1,1:4].mean()\n",
    "\n",
    "    general_agreement_df = pd.concat([general_agreement_df,\n",
    "                                      pd.DataFrame(data=[['Difference from mean human annotators'] + (general_agreement_df.iloc[-1, 1:] - human_mean).to_list()], columns=general_agreement_df.columns)])\n",
    "\n",
    "    return general_agreement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18da59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations_all(evaluation_df, which_score, correlation_fn):\n",
    "    \n",
    "    general_agreement_df = pd.DataFrame()\n",
    "\n",
    "    general_agreement_df['comparisson'] = ['a1', 'a2', 'a3']\n",
    "\n",
    "    general_agreement_df['a1'] = [np.nan, \n",
    "                                  globals()[correlation_fn](a1_df[which_score], a2_df[which_score])[0], \n",
    "                                  globals()[correlation_fn](a1_df[which_score], a3_df[which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a2'] = [globals()[correlation_fn](a1_df[which_score], a2_df[which_score])[0], \n",
    "                                  np.nan, \n",
    "                                  globals()[correlation_fn](a2_df[which_score], a3_df[which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a3'] = [globals()[correlation_fn](a1_df[which_score], a3_df[which_score])[0], \n",
    "                                  globals()[correlation_fn](a2_df[which_score], a3_df[which_score])[0], \n",
    "                                  np.nan]\n",
    "\n",
    "    general_agreement_df['GPT4 4-score'] = [globals()[correlation_fn](a1_df[which_score], evaluation_df[which_score])[0], \n",
    "                                            globals()[correlation_fn](a2_df[which_score], evaluation_df[which_score])[0], \n",
    "                                            globals()[correlation_fn](a3_df[which_score], evaluation_df[which_score])[0]]\n",
    "\n",
    "    general_agreement_df = pd.concat([general_agreement_df, \n",
    "                                      pd.DataFrame(data=[['mean'] + general_agreement_df.iloc[:, 1:].mean().to_list()], columns=general_agreement_df.columns)])\n",
    "\n",
    "    general_agreement_df = pd.concat([general_agreement_df, \n",
    "                                      pd.DataFrame(data=[['std'] + general_agreement_df.iloc[:, 1:].std().to_list()], columns=general_agreement_df.columns)])\n",
    "    \n",
    "    \n",
    "    human_mean = general_agreement_df.iloc[-1,1:4].mean()\n",
    "\n",
    "    general_agreement_df = pd.concat([general_agreement_df,\n",
    "                                      pd.DataFrame(data=[['Difference from mean human annotators'] + (general_agreement_df.iloc[-1, 1:] - human_mean).to_list()], columns=general_agreement_df.columns)])\n",
    "    \n",
    "    return general_agreement_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e61ad0",
   "metadata": {},
   "source": [
    "### Functions to compute correlation against single score computed accross human annoators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1dac117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations_single_score_1st_set(evaluation_df, which_score, correlation_fn):\n",
    "\n",
    "    general_agreement_df = pd.DataFrame()\n",
    "\n",
    "    general_agreement_df['comparisson'] = ['single']\n",
    "\n",
    "    general_agreement_df['a1'] = [globals()[correlation_fn](a1_df.iloc[0:120][which_score], single_score_df.iloc[0:120][which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a2'] = [globals()[correlation_fn](a2_df.iloc[0:120][which_score], single_score_df.iloc[0:120][which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a3'] = [globals()[correlation_fn](a3_df.iloc[0:120][which_score], single_score_df.iloc[0:120][which_score])[0]]\n",
    "    \n",
    "    human_mean = general_agreement_df.iloc[-1,1:4].mean()\n",
    "    \n",
    "    general_agreement_df['human mean'] = human_mean\n",
    "    \n",
    "    general_agreement_df['human std'] = general_agreement_df.iloc[-1,1:4].std()\n",
    "\n",
    "    general_agreement_df['GPT4 1st 4-score'] = [globals()[correlation_fn](single_score_df.iloc[0:120][which_score], evaluation_df[which_score])[0]]\n",
    "\n",
    "    difference_from_mean = ['Difference from mean human annotators'] + (general_agreement_df.iloc[-1, 1:] - human_mean).to_list()\n",
    "    difference_from_mean[-2] = np.nan\n",
    "    \n",
    "    general_agreement_df = pd.concat([general_agreement_df,\n",
    "                                      pd.DataFrame(data=[difference_from_mean], columns=general_agreement_df.columns)])\n",
    "\n",
    "    return general_agreement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fcd6dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations_single_score_2nd_set(evaluation_df, which_score, correlation_fn):\n",
    "\n",
    "    general_agreement_df = pd.DataFrame()\n",
    "\n",
    "    general_agreement_df['comparisson'] = ['single']\n",
    "\n",
    "    general_agreement_df['a1'] = [globals()[correlation_fn](a1_df.iloc[120:240][which_score], single_score_df.iloc[120:240][which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a2'] = [globals()[correlation_fn](a2_df.iloc[120:240][which_score], single_score_df.iloc[120:240][which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a3'] = [globals()[correlation_fn](a3_df.iloc[120:240][which_score], single_score_df.iloc[120:240][which_score])[0]]\n",
    "    \n",
    "    human_mean = general_agreement_df.iloc[-1,1:4].mean()\n",
    "    \n",
    "    general_agreement_df['human mean'] = human_mean\n",
    "    \n",
    "    general_agreement_df['human std'] = general_agreement_df.iloc[-1,1:4].std()\n",
    "\n",
    "    general_agreement_df['GPT4 1st 4-score'] = [globals()[correlation_fn](single_score_df.iloc[120:240][which_score], evaluation_df[which_score])[0]]\n",
    "\n",
    "    difference_from_mean = ['Difference from mean human annotators'] + (general_agreement_df.iloc[-1, 1:] - human_mean).to_list()\n",
    "    difference_from_mean[-2] = np.nan\n",
    "    \n",
    "    general_agreement_df = pd.concat([general_agreement_df,\n",
    "                                      pd.DataFrame(data=[difference_from_mean], columns=general_agreement_df.columns)])\n",
    "\n",
    "    return general_agreement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4392871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations_single_score_all(evaluation_df, which_score, correlation_fn):\n",
    "\n",
    "    general_agreement_df = pd.DataFrame()\n",
    "\n",
    "    general_agreement_df['comparisson'] = ['single']\n",
    "\n",
    "    general_agreement_df['a1'] = [globals()[correlation_fn](a1_df[which_score], single_score_df[which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a2'] = [globals()[correlation_fn](a2_df[which_score], single_score_df[which_score])[0]]\n",
    "\n",
    "    general_agreement_df['a3'] = [globals()[correlation_fn](a3_df[which_score], single_score_df[which_score])[0]]\n",
    "    \n",
    "    human_mean = general_agreement_df.iloc[-1,1:4].mean()\n",
    "    \n",
    "    general_agreement_df['human mean'] = human_mean\n",
    "    \n",
    "    general_agreement_df['human std'] = general_agreement_df.iloc[-1,1:4].std()\n",
    "\n",
    "    general_agreement_df['GPT4 1st 4-score'] = [globals()[correlation_fn](single_score_df[which_score], evaluation_df[which_score])[0]]\n",
    "\n",
    "    difference_from_mean = ['Difference from mean human annotators'] + (general_agreement_df.iloc[-1, 1:] - human_mean).to_list()\n",
    "    difference_from_mean[-2] = np.nan\n",
    "    \n",
    "    general_agreement_df = pd.concat([general_agreement_df,\n",
    "                                      pd.DataFrame(data=[difference_from_mean], columns=general_agreement_df.columns)])\n",
    "\n",
    "    return general_agreement_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5fbc3",
   "metadata": {},
   "source": [
    "## Check the regenerated evaluations, comparing against the original GPT-4 4-score evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14062379",
   "metadata": {},
   "source": [
    "### Evaluation using the new GPT-4 turbo ― Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bcdcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_1106_1st_df = pd.read_csv(os.path.join(\"tests\", \"test_000_119_gpt-4-1106-preview_20231108_fixed_2_scores.tsv\"), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3c49b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_1106_2nd_df = pd.read_csv(os.path.join(\"tests\", \"test_120_239_gpt-4-1106-preview_20231115_fixed_2_scores.tsv\"), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f670405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_1106_df = pd.concat([gpt4_1106_1st_df, gpt4_1106_2nd_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1474f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.706979</td>\n",
       "      <td>0.817080</td>\n",
       "      <td>0.655061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.706979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.752948</td>\n",
       "      <td>0.657686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.817080</td>\n",
       "      <td>0.752948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.656545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.762030</td>\n",
       "      <td>0.729963</td>\n",
       "      <td>0.785014</td>\n",
       "      <td>0.656431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.055051</td>\n",
       "      <td>0.022985</td>\n",
       "      <td>0.032066</td>\n",
       "      <td>0.001075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.018350</td>\n",
       "      <td>-0.013716</td>\n",
       "      <td>-0.004634</td>\n",
       "      <td>-0.035626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2        a3  \\\n",
       "0                                     a1      NaN  0.706979  0.817080   \n",
       "1                                     a2 0.706979       NaN  0.752948   \n",
       "2                                     a3 0.817080  0.752948       NaN   \n",
       "0                                   mean 0.762030  0.729963  0.785014   \n",
       "0                                    std 0.055051  0.022985  0.032066   \n",
       "0  Difference from mean human annotators 0.018350 -0.013716 -0.004634   \n",
       "\n",
       "   GPT4 1st 4-score  \n",
       "0          0.655061  \n",
       "1          0.657686  \n",
       "2          0.656545  \n",
       "0          0.656431  \n",
       "0          0.001075  \n",
       "0         -0.035626  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_1st_set(gpt4_1106_1st_df, 'score', 'spearmanr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67eb97bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 2nd 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.663906</td>\n",
       "      <td>0.578996</td>\n",
       "      <td>0.576091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.663906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.679524</td>\n",
       "      <td>0.607351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.578996</td>\n",
       "      <td>0.679524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.602190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.621451</td>\n",
       "      <td>0.671715</td>\n",
       "      <td>0.629260</td>\n",
       "      <td>0.595211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.042455</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>0.050264</td>\n",
       "      <td>0.013683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>-0.025700</td>\n",
       "      <td>0.016755</td>\n",
       "      <td>-0.019827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2       a3  \\\n",
       "0                                     a1      NaN  0.663906 0.578996   \n",
       "1                                     a2 0.663906       NaN 0.679524   \n",
       "2                                     a3 0.578996  0.679524      NaN   \n",
       "0                                   mean 0.621451  0.671715 0.629260   \n",
       "0                                    std 0.042455  0.007809 0.050264   \n",
       "0  Difference from mean human annotators 0.008945 -0.025700 0.016755   \n",
       "\n",
       "   GPT4 2nd 4-score  \n",
       "0          0.576091  \n",
       "1          0.607351  \n",
       "2          0.602190  \n",
       "0          0.595211  \n",
       "0          0.013683  \n",
       "0         -0.019827  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_2nd_set(gpt4_1106_2nd_df, 'score', 'spearmanr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2534ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.693098</td>\n",
       "      <td>0.692357</td>\n",
       "      <td>0.607279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.693098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.698475</td>\n",
       "      <td>0.617434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.692357</td>\n",
       "      <td>0.698475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.629563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.692728</td>\n",
       "      <td>0.695787</td>\n",
       "      <td>0.695416</td>\n",
       "      <td>0.618092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.009109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>-0.001669</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.007070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson        a1       a2       a3  \\\n",
       "0                                     a1       NaN 0.693098 0.692357   \n",
       "1                                     a2  0.693098      NaN 0.698475   \n",
       "2                                     a3  0.692357 0.698475      NaN   \n",
       "0                                   mean  0.692728 0.695787 0.695416   \n",
       "0                                    std  0.000370 0.002689 0.003059   \n",
       "0  Difference from mean human annotators -0.001669 0.000649 0.001020   \n",
       "\n",
       "   GPT4 4-score  \n",
       "0      0.607279  \n",
       "1      0.617434  \n",
       "2      0.629563  \n",
       "0      0.618092  \n",
       "0      0.009109  \n",
       "0      0.007070  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_all(gpt4_1106_df, 'score', 'spearmanr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b33ba",
   "metadata": {},
   "source": [
    "### Evaluation using the new GPT-4 turbo ― Cohen Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6274fc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.489892</td>\n",
       "      <td>0.575498</td>\n",
       "      <td>0.404606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.489892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500324</td>\n",
       "      <td>0.363543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.575498</td>\n",
       "      <td>0.500324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.445074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.532695</td>\n",
       "      <td>0.495108</td>\n",
       "      <td>0.537911</td>\n",
       "      <td>0.404408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.037587</td>\n",
       "      <td>0.033285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.014268</td>\n",
       "      <td>-0.023319</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>0.004750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2       a3  \\\n",
       "0                                     a1      NaN  0.489892 0.575498   \n",
       "1                                     a2 0.489892       NaN 0.500324   \n",
       "2                                     a3 0.575498  0.500324      NaN   \n",
       "0                                   mean 0.532695  0.495108 0.537911   \n",
       "0                                    std 0.042803  0.005216 0.037587   \n",
       "0  Difference from mean human annotators 0.014268 -0.023319 0.009052   \n",
       "\n",
       "   GPT4 1st 4-score  \n",
       "0          0.404606  \n",
       "1          0.363543  \n",
       "2          0.445074  \n",
       "0          0.404408  \n",
       "0          0.033285  \n",
       "0          0.004750  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_1st_set(gpt4_1106_1st_df, 'score', 'cohen_kappa_wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adb26b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 2nd 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.369369</td>\n",
       "      <td>0.286629</td>\n",
       "      <td>0.243283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.369369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.322651</td>\n",
       "      <td>0.157364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.286629</td>\n",
       "      <td>0.322651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.327999</td>\n",
       "      <td>0.346010</td>\n",
       "      <td>0.304640</td>\n",
       "      <td>0.218529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.041370</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.018011</td>\n",
       "      <td>0.043511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.013790</td>\n",
       "      <td>-0.004221</td>\n",
       "      <td>-0.009569</td>\n",
       "      <td>0.015931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2        a3  \\\n",
       "0                                     a1      NaN  0.369369  0.286629   \n",
       "1                                     a2 0.369369       NaN  0.322651   \n",
       "2                                     a3 0.286629  0.322651       NaN   \n",
       "0                                   mean 0.327999  0.346010  0.304640   \n",
       "0                                    std 0.041370  0.023359  0.018011   \n",
       "0  Difference from mean human annotators 0.013790 -0.004221 -0.009569   \n",
       "\n",
       "   GPT4 2nd 4-score  \n",
       "0          0.243283  \n",
       "1          0.157364  \n",
       "2          0.254939  \n",
       "0          0.218529  \n",
       "0          0.043511  \n",
       "0          0.015931  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_2nd_set(gpt4_1106_2nd_df, 'score', 'cohen_kappa_wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9155f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.436881</td>\n",
       "      <td>0.429402</td>\n",
       "      <td>0.323428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.436881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.410455</td>\n",
       "      <td>0.259276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.429402</td>\n",
       "      <td>0.410455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.349839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.433142</td>\n",
       "      <td>0.423668</td>\n",
       "      <td>0.419929</td>\n",
       "      <td>0.310848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.013213</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.038027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>-0.005069</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.029218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson        a1       a2       a3  \\\n",
       "0                                     a1       NaN 0.436881 0.429402   \n",
       "1                                     a2  0.436881      NaN 0.410455   \n",
       "2                                     a3  0.429402 0.410455      NaN   \n",
       "0                                   mean  0.433142 0.423668 0.419929   \n",
       "0                                    std  0.003740 0.013213 0.009474   \n",
       "0  Difference from mean human annotators -0.005069 0.004404 0.000665   \n",
       "\n",
       "   GPT4 4-score  \n",
       "0      0.323428  \n",
       "1      0.259276  \n",
       "2      0.349839  \n",
       "0      0.310848  \n",
       "0      0.038027  \n",
       "0      0.029218  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_all(gpt4_1106_df, 'score', 'cohen_kappa_wrapper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df197f3b",
   "metadata": {},
   "source": [
    "### Evaluation using the new GPT-4 turbo ― Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c2843dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.719862</td>\n",
       "      <td>0.825060</td>\n",
       "      <td>0.650404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.719862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.768756</td>\n",
       "      <td>0.657615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.825060</td>\n",
       "      <td>0.768756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.657503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.772461</td>\n",
       "      <td>0.744309</td>\n",
       "      <td>0.796908</td>\n",
       "      <td>0.655174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.052599</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>0.028152</td>\n",
       "      <td>0.003373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.017533</td>\n",
       "      <td>-0.010619</td>\n",
       "      <td>-0.006914</td>\n",
       "      <td>-0.031693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2        a3  \\\n",
       "0                                     a1      NaN  0.719862  0.825060   \n",
       "1                                     a2 0.719862       NaN  0.768756   \n",
       "2                                     a3 0.825060  0.768756       NaN   \n",
       "0                                   mean 0.772461  0.744309  0.796908   \n",
       "0                                    std 0.052599  0.024447  0.028152   \n",
       "0  Difference from mean human annotators 0.017533 -0.010619 -0.006914   \n",
       "\n",
       "   GPT4 1st 4-score  \n",
       "0          0.650404  \n",
       "1          0.657615  \n",
       "2          0.657503  \n",
       "0          0.655174  \n",
       "0          0.003373  \n",
       "0         -0.031693  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_1st_set(gpt4_1106_1st_df, 'score', 'pearsonr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0924cbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 2nd 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.657464</td>\n",
       "      <td>0.576163</td>\n",
       "      <td>0.553798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.657464</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.687592</td>\n",
       "      <td>0.595538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.576163</td>\n",
       "      <td>0.687592</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.607573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.616813</td>\n",
       "      <td>0.672528</td>\n",
       "      <td>0.631878</td>\n",
       "      <td>0.585636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.055715</td>\n",
       "      <td>0.023043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>-0.022079</td>\n",
       "      <td>0.018572</td>\n",
       "      <td>-0.014100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2       a3  \\\n",
       "0                                     a1      NaN  0.657464 0.576163   \n",
       "1                                     a2 0.657464       NaN 0.687592   \n",
       "2                                     a3 0.576163  0.687592      NaN   \n",
       "0                                   mean 0.616813  0.672528 0.631878   \n",
       "0                                    std 0.040650  0.015064 0.055715   \n",
       "0  Difference from mean human annotators 0.003507 -0.022079 0.018572   \n",
       "\n",
       "   GPT4 2nd 4-score  \n",
       "0          0.553798  \n",
       "1          0.595538  \n",
       "2          0.607573  \n",
       "0          0.585636  \n",
       "0          0.023043  \n",
       "0         -0.014100  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_2nd_set(gpt4_1106_2nd_df, 'score', 'pearsonr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a558511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.698174</td>\n",
       "      <td>0.697289</td>\n",
       "      <td>0.598166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.698174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.713168</td>\n",
       "      <td>0.614565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.697289</td>\n",
       "      <td>0.713168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.632562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.697732</td>\n",
       "      <td>0.705671</td>\n",
       "      <td>0.705229</td>\n",
       "      <td>0.615098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.014047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>-0.004851</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.008754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson        a1       a2       a3  \\\n",
       "0                                     a1       NaN 0.698174 0.697289   \n",
       "1                                     a2  0.698174      NaN 0.713168   \n",
       "2                                     a3  0.697289 0.713168      NaN   \n",
       "0                                   mean  0.697732 0.705671 0.705229   \n",
       "0                                    std  0.000442 0.007497 0.007939   \n",
       "0  Difference from mean human annotators -0.004851 0.002204 0.002646   \n",
       "\n",
       "   GPT4 4-score  \n",
       "0      0.598166  \n",
       "1      0.614565  \n",
       "2      0.632562  \n",
       "0      0.615098  \n",
       "0      0.014047  \n",
       "0      0.008754  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_all(gpt4_1106_df, 'score', 'pearsonr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f5da2",
   "metadata": {},
   "source": [
    "### Evaluation using the new GPT-4 turbo ― Kendall Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25838d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.643791</td>\n",
       "      <td>0.751429</td>\n",
       "      <td>0.582366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.643791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.691653</td>\n",
       "      <td>0.584938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.751429</td>\n",
       "      <td>0.691653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.581670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.697610</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.721541</td>\n",
       "      <td>0.582991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.053819</td>\n",
       "      <td>0.023931</td>\n",
       "      <td>0.029888</td>\n",
       "      <td>0.001405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.017940</td>\n",
       "      <td>-0.011948</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>-0.034474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2        a3  \\\n",
       "0                                     a1      NaN  0.643791  0.751429   \n",
       "1                                     a2 0.643791       NaN  0.691653   \n",
       "2                                     a3 0.751429  0.691653       NaN   \n",
       "0                                   mean 0.697610  0.667722  0.721541   \n",
       "0                                    std 0.053819  0.023931  0.029888   \n",
       "0  Difference from mean human annotators 0.017940 -0.011948 -0.005991   \n",
       "\n",
       "   GPT4 1st 4-score  \n",
       "0          0.582366  \n",
       "1          0.584938  \n",
       "2          0.581670  \n",
       "0          0.582991  \n",
       "0          0.001405  \n",
       "0         -0.034474  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_1st_set(gpt4_1106_1st_df, 'score', 'kendalltau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96369030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 2nd 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.590647</td>\n",
       "      <td>0.510594</td>\n",
       "      <td>0.491843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.590647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600221</td>\n",
       "      <td>0.535608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.510594</td>\n",
       "      <td>0.600221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.510917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.550620</td>\n",
       "      <td>0.595434</td>\n",
       "      <td>0.555407</td>\n",
       "      <td>0.512789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.040027</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>0.044814</td>\n",
       "      <td>0.017916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>-0.025089</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>-0.011960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2       a3  \\\n",
       "0                                     a1      NaN  0.590647 0.510594   \n",
       "1                                     a2 0.590647       NaN 0.600221   \n",
       "2                                     a3 0.510594  0.600221      NaN   \n",
       "0                                   mean 0.550620  0.595434 0.555407   \n",
       "0                                    std 0.040027  0.004787 0.044814   \n",
       "0  Difference from mean human annotators 0.010151 -0.025089 0.014938   \n",
       "\n",
       "   GPT4 2nd 4-score  \n",
       "0          0.491843  \n",
       "1          0.535608  \n",
       "2          0.510917  \n",
       "0          0.512789  \n",
       "0          0.017916  \n",
       "0         -0.011960  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_2nd_set(gpt4_1106_2nd_df, 'score', 'kendalltau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71968beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>GPT4 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.623709</td>\n",
       "      <td>0.622409</td>\n",
       "      <td>0.529502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>0.623709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.626314</td>\n",
       "      <td>0.539189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>0.622409</td>\n",
       "      <td>0.626314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.546029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.623059</td>\n",
       "      <td>0.625011</td>\n",
       "      <td>0.624362</td>\n",
       "      <td>0.538240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>std</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.006780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.005479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson        a1       a2       a3  \\\n",
       "0                                     a1       NaN 0.623709 0.622409   \n",
       "1                                     a2  0.623709      NaN 0.626314   \n",
       "2                                     a3  0.622409 0.626314      NaN   \n",
       "0                                   mean  0.623059 0.625011 0.624362   \n",
       "0                                    std  0.000650 0.001303 0.001953   \n",
       "0  Difference from mean human annotators -0.000652 0.000001 0.000651   \n",
       "\n",
       "   GPT4 4-score  \n",
       "0      0.529502  \n",
       "1      0.539189  \n",
       "2      0.546029  \n",
       "0      0.538240  \n",
       "0      0.006780  \n",
       "0      0.005479  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_all(gpt4_1106_df, 'score', 'kendalltau')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b69c7c",
   "metadata": {},
   "source": [
    "# Check agreement against single score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899716b",
   "metadata": {},
   "source": [
    "### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d39a1f79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>human mean</th>\n",
       "      <th>human std</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>single</td>\n",
       "      <td>0.907080</td>\n",
       "      <td>0.815884</td>\n",
       "      <td>0.893411</td>\n",
       "      <td>0.872125</td>\n",
       "      <td>0.049183</td>\n",
       "      <td>0.650659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.034955</td>\n",
       "      <td>-0.056241</td>\n",
       "      <td>0.021286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.221466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2       a3  \\\n",
       "0                                 single 0.907080  0.815884 0.893411   \n",
       "0  Difference from mean human annotators 0.034955 -0.056241 0.021286   \n",
       "\n",
       "   human mean  human std  GPT4 1st 4-score  \n",
       "0    0.872125   0.049183          0.650659  \n",
       "0    0.000000        NaN         -0.221466  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_single_score_1st_set(gpt4_1106_1st_df, 'score', 'spearmanr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c9770773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>human mean</th>\n",
       "      <th>human std</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>single</td>\n",
       "      <td>0.795646</td>\n",
       "      <td>0.832723</td>\n",
       "      <td>0.755262</td>\n",
       "      <td>0.794544</td>\n",
       "      <td>0.038742</td>\n",
       "      <td>0.618513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.038179</td>\n",
       "      <td>-0.039281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.176031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1       a2        a3  \\\n",
       "0                                 single 0.795646 0.832723  0.755262   \n",
       "0  Difference from mean human annotators 0.001102 0.038179 -0.039281   \n",
       "\n",
       "   human mean  human std  GPT4 1st 4-score  \n",
       "0    0.794544   0.038742          0.618513  \n",
       "0    0.000000        NaN         -0.176031  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_single_score_2nd_set(gpt4_1106_2nd_df, 'score', 'spearmanr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "52c179cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>human mean</th>\n",
       "      <th>human std</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>single</td>\n",
       "      <td>0.855277</td>\n",
       "      <td>0.825678</td>\n",
       "      <td>0.817974</td>\n",
       "      <td>0.832976</td>\n",
       "      <td>0.019694</td>\n",
       "      <td>0.626833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.022301</td>\n",
       "      <td>-0.007299</td>\n",
       "      <td>-0.015003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.206143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2        a3  \\\n",
       "0                                 single 0.855277  0.825678  0.817974   \n",
       "0  Difference from mean human annotators 0.022301 -0.007299 -0.015003   \n",
       "\n",
       "   human mean  human std  GPT4 1st 4-score  \n",
       "0    0.832976   0.019694          0.626833  \n",
       "0    0.000000        NaN         -0.206143  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_single_score_all(gpt4_1106_df, 'score', 'spearmanr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9bc111",
   "metadata": {},
   "source": [
    "### Cohen Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cd9f2e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>human mean</th>\n",
       "      <th>human std</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>single</td>\n",
       "      <td>0.809203</td>\n",
       "      <td>0.680821</td>\n",
       "      <td>0.754304</td>\n",
       "      <td>0.748109</td>\n",
       "      <td>0.064415</td>\n",
       "      <td>0.461431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.061094</td>\n",
       "      <td>-0.067289</td>\n",
       "      <td>0.006195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.286679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1        a2       a3  \\\n",
       "0                                 single 0.809203  0.680821 0.754304   \n",
       "0  Difference from mean human annotators 0.061094 -0.067289 0.006195   \n",
       "\n",
       "   human mean  human std  GPT4 1st 4-score  \n",
       "0    0.748109   0.064415          0.461431  \n",
       "0    0.000000        NaN         -0.286679  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_single_score_1st_set(gpt4_1106_1st_df, 'score', 'cohen_kappa_wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4bfafa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>human mean</th>\n",
       "      <th>human std</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>single</td>\n",
       "      <td>0.629734</td>\n",
       "      <td>0.696117</td>\n",
       "      <td>0.559310</td>\n",
       "      <td>0.628387</td>\n",
       "      <td>0.068414</td>\n",
       "      <td>0.210021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>-0.069077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.418366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1       a2        a3  \\\n",
       "0                                 single 0.629734 0.696117  0.559310   \n",
       "0  Difference from mean human annotators 0.001347 0.067730 -0.069077   \n",
       "\n",
       "   human mean  human std  GPT4 1st 4-score  \n",
       "0    0.628387   0.068414          0.210021  \n",
       "0    0.000000        NaN         -0.418366  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_single_score_2nd_set(gpt4_1106_2nd_df, 'score', 'cohen_kappa_wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1288c69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comparisson</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>human mean</th>\n",
       "      <th>human std</th>\n",
       "      <th>GPT4 1st 4-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>single</td>\n",
       "      <td>0.722106</td>\n",
       "      <td>0.692092</td>\n",
       "      <td>0.655978</td>\n",
       "      <td>0.690059</td>\n",
       "      <td>0.033111</td>\n",
       "      <td>0.334673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Difference from mean human annotators</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>-0.034081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.355386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comparisson       a1       a2        a3  \\\n",
       "0                                 single 0.722106 0.692092  0.655978   \n",
       "0  Difference from mean human annotators 0.032048 0.002034 -0.034081   \n",
       "\n",
       "   human mean  human std  GPT4 1st 4-score  \n",
       "0    0.690059   0.033111          0.334673  \n",
       "0    0.000000        NaN         -0.355386  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_single_score_all(gpt4_1106_df, 'score', 'cohen_kappa_wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aea88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d59e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85661198",
   "metadata": {},
   "source": [
    "# Check the agreement per question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d01b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_per_question(which_score):\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    res_a1_a2 = check_agreement_per_questions(a1_df, a2_df, suffix=\"_a1_a2\", score=which_score)\n",
    "    res_a1_a3 = check_agreement_per_questions(a1_df, a3_df, suffix=\"_a1_a3\", score=which_score)\n",
    "    res_a2_a3 = check_agreement_per_questions(a2_df, a3_df, suffix=\"_a2_a3\", score=which_score)\n",
    "\n",
    "    res_a1_gpt4 = check_agreement_per_questions(a1_df, gpt4_df, \"_a1_gpt4\", score=which_score)\n",
    "    res_a2_gpt4 = check_agreement_per_questions(a2_df, gpt4_df, \"_a2_gpt4\", score=which_score)\n",
    "    res_a3_gpt4 = check_agreement_per_questions(a3_df, gpt4_df, \"_a3_gpt4\", score=which_score)\n",
    "\n",
    "    res_a1_gpt4_cot_2_6 = check_agreement_per_questions(a1_df, gpt4_cot_2_6_df, \"_a1_gpt4_cot_2.6\", score=which_score)\n",
    "    res_a2_gpt4_cot_2_6 = check_agreement_per_questions(a2_df, gpt4_cot_2_6_df, \"_a2_gpt4_cot_2.6\", score=which_score)\n",
    "    res_a3_gpt4_cot_2_6 = check_agreement_per_questions(a3_df, gpt4_cot_2_6_df, \"_a3_gpt4_cot_2.6\", score=which_score)\n",
    "\n",
    "    if which_score != 'score':\n",
    "        res_a1_gpt4_2_7 = check_agreement_per_questions(a1_df, gpt4_03_score_2_7_df, \"_a1_gpt4_2.7\", score=which_score)\n",
    "        res_a2_gpt4_2_7 = check_agreement_per_questions(a2_df, gpt4_03_score_2_7_df, \"_a2_gpt4_2.7\", score=which_score)\n",
    "        res_a3_gpt4_2_7 = check_agreement_per_questions(a3_df, gpt4_03_score_2_7_df, \"_a3_gpt4_2.7\", score=which_score)\n",
    "\n",
    "        res_a1_gpt4_2_9 = check_agreement_per_questions(a1_df, gpt4_03_score_2_9_df, \"_a1_gpt4_2.9\", score=which_score)\n",
    "        res_a2_gpt4_2_9 = check_agreement_per_questions(a2_df, gpt4_03_score_2_9_df, \"_a2_gpt4_2.9\", score=which_score)\n",
    "        res_a3_gpt4_2_9 = check_agreement_per_questions(a3_df, gpt4_03_score_2_9_df, \"_a3_gpt4_2.9\", score=which_score)\n",
    "    \n",
    "    \n",
    "    human_combined_res = pd.concat([res_a1_a2[1], res_a1_a3[1].iloc[:, -1], res_a2_a3[1].iloc[:, -1]], axis=1)\n",
    "\n",
    "    human_combined_res['cohen_kappa_mean'] = human_combined_res.iloc[:, 1:].mean(axis=1)\n",
    "    human_combined_res['cohen_kappa_std'] = human_combined_res.iloc[:, 1:].std(axis=1)\n",
    "    \n",
    "    all_results.append(human_combined_res)\n",
    "    \n",
    "    human_gpt4_combined_res = pd.concat([res_a1_gpt4[1], res_a2_gpt4[1].iloc[:, -1], res_a3_gpt4[1].iloc[:, -1]], axis=1)\n",
    "    human_gpt4_combined_res['cohen_kappa_mean'] = human_gpt4_combined_res.iloc[:, 1:].mean(axis=1)\n",
    "    human_gpt4_combined_res['cohen_kappa_std'] = human_gpt4_combined_res.iloc[:, 1:].std(axis=1)\n",
    "\n",
    "    all_results.append(human_gpt4_combined_res)    \n",
    "    \n",
    "    human_gpt4_cot_2_6_combined_res = pd.concat([res_a1_gpt4_cot_2_6[1], res_a2_gpt4_cot_2_6[1].iloc[:, -1], res_a3_gpt4_cot_2_6[1].iloc[:, -1]], axis=1)\n",
    "    human_gpt4_cot_2_6_combined_res['cohen_kappa_mean'] = human_gpt4_cot_2_6_combined_res.iloc[:, 1:].mean(axis=1)\n",
    "    human_gpt4_cot_2_6_combined_res['cohen_kappa_std'] = human_gpt4_cot_2_6_combined_res.iloc[:, 1:].std(axis=1)\n",
    "\n",
    "    all_results.append(human_gpt4_cot_2_6_combined_res)    \n",
    "\n",
    "    if which_score != 'score':\n",
    "        human_gpt4_2_7_combined_res = pd.concat([res_a1_gpt4_2_7[1], res_a2_gpt4_2_7[1].iloc[:, -1], res_a3_gpt4_2_7[1].iloc[:, -1]], axis=1)\n",
    "        human_gpt4_2_7_combined_res['cohen_kappa_mean'] = human_gpt4_2_7_combined_res.iloc[:, 1:].mean(axis=1)\n",
    "        human_gpt4_2_7_combined_res['cohen_kappa_std'] = human_gpt4_2_7_combined_res.iloc[:, 1:].std(axis=1)\n",
    "\n",
    "        all_results.append(human_gpt4_2_7_combined_res)    \n",
    "        \n",
    "        human_gpt4_2_9_combined_res = pd.concat([res_a1_gpt4_2_9[1], res_a2_gpt4_2_9[1].iloc[:, -1], res_a3_gpt4_2_9[1].iloc[:, -1]], axis=1)\n",
    "        human_gpt4_2_9_combined_res['cohen_kappa_mean'] = human_gpt4_2_9_combined_res.iloc[:, 1:].mean(axis=1)\n",
    "        human_gpt4_2_9_combined_res['cohen_kappa_std'] = human_gpt4_2_9_combined_res.iloc[:, 1:].std(axis=1)\n",
    "    \n",
    "        all_results.append(human_gpt4_2_9_combined_res)    \n",
    "    \n",
    "    \n",
    "    correlations = []\n",
    "    correlations.append({'annotator': 'human',\n",
    "                         'cohen_kappa': human_combined_res['cohen_kappa_mean'].mean()})\n",
    "    \n",
    "    correlations.append({'annotator': 'GPT-4',\n",
    "                         'cohen_kappa': human_gpt4_combined_res['cohen_kappa_mean'].mean()})\n",
    "    correlations.append({'annotator': 'GPT-4 CoT 2.6',\n",
    "                         'cohen_kappa': human_gpt4_cot_2_6_combined_res['cohen_kappa_mean'].mean()})\n",
    "\n",
    "    if which_score != 'score':\n",
    "        correlations.append({'annotator': 'GPT-4 2.7',\n",
    "                             'cohen_kappa': human_gpt4_2_7_combined_res['cohen_kappa_mean'].mean()})\n",
    "        correlations.append({'annotator': 'GPT-4 2.9',\n",
    "                             'cohen_kappa': human_gpt4_2_9_combined_res['cohen_kappa_mean'].mean()})\n",
    "        \n",
    "    \n",
    "    \n",
    "    correlations_df = pd.DataFrame(correlations)\n",
    "    \n",
    "    correlations_df['difference'] = correlations_df['cohen_kappa'] - human_combined_res['cohen_kappa_mean'].mean()\n",
    "    \n",
    "    display(correlations_df)\n",
    "    \n",
    "    return correlations_df, all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ca4f7",
   "metadata": {},
   "source": [
    "### Check for 4-score evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e861e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = correlation_per_question('score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb37402",
   "metadata": {},
   "source": [
    "### Check for 3-score evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c5778",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_3_df = correlation_per_question('03_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11baac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_2_df = correlation_per_question('02_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18498a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_2a_df = correlation_per_question('02_score_alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6e19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
